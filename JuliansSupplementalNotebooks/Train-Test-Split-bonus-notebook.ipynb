{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Splits\n",
    "\n",
    "### The Problem\n",
    "\n",
    "When building a predictive model we can run into a problem of overfitting. \n",
    "\n",
    "**OVERFITTING WILL BE A MAJOR TOPIC WE WILL DEAL WITH FOR AS LONG AS WE MAKE PREDICTIVE MODELS**\n",
    "\n",
    "Overfitting is a model that does *too well* on the data we've given in, so much so that it makes the model *worse* at looking at new data.\n",
    "\n",
    "Take this data for example:\n",
    "\n",
    "![Clearly linear scatterplot](Images/justdot.png)\n",
    "\n",
    "It's our data is pretty clearly linear, a linear model wouldn't prefectly predict every point, but it would be pretty good.\n",
    "\n",
    "![Simple linear model](Images/propermodel.png)\n",
    "\n",
    "Buuuuuuuut I could totally write an algorithm or funtion that perfectly hits every point here.\n",
    "\n",
    "!['Better' model](Images/overfitmodel.png)\n",
    "\n",
    "\n",
    "This second model will perform better on any metrics we feed into it... FOR THESE SPECIFIC DATA POINTS. \n",
    "\n",
    "But, we want models that make predictions. That's 99% of the point of data science. \n",
    "\n",
    "If we tried to feed our models new information (e.g. use the model in the real world) the 'worse' model will perform much better.\n",
    "\n",
    "![New data comparison](Images/withnewdata.png)\n",
    "\n",
    "\n",
    "\n",
    "So how do we know if our model is overfit? We can't magically create new data, but we can HIDE data from the model, only to reveal it later. This is the train-test split. We train the models on one set of data, we verify the model's performance on the test data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting an example dataset\n",
    "\n",
    "Let's grab a dataset and see how this works. I'm just grabbing the diabetes dataset from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#first we load the dataset. load_diabetes has an option to load directly as a pandas df\n",
    "\n",
    "diabetes_df = datasets.load_diabetes(as_frame=True, scaled = False).frame\n",
    "# choosing 'as_frame = True' makes the dataset a pandas DF, setting scaled = False\n",
    "# means that we get the raw data, and I'll talk about why I did that later.\n",
    "# NOTE! That's not an option in the version of sklearn y'all have.\n",
    "\n",
    "diabetes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the FIRST thing we want to do is the train-test-split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the part of the lecture where I verbally explain all the options for train_test_split\n",
    "train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The options to input into train_test_split:\n",
    "\n",
    "##### arrays\n",
    "First, we MUST pass in arrays. They must be \"sequence of indexables with same length / shape\" That is, the same number of rows, and those rows must match up. So row 1 in one array needs to map to row 1 in the second array.  In this case, our X variables or predictors, and our Y variables or target. \n",
    "\n",
    "##### test_size\n",
    "Next, we can choose the test_size. We can set this manually, or use the default. The default is .25, or 25% of the data. If we pass an int it will set the size of the test size to that int, if we pass a value between 1 and 0 it makes the test dataset a corresponding % of the overall dataset. The default here is fine for most use cases.\n",
    "\n",
    "##### train_size\n",
    "Or we can set the training size instead of the test size. But usually we want to use all the data we don't put into the test set, so by default we leave this untouched.\n",
    "\n",
    "##### random_state\n",
    "So, we want our tts to be randomized, otherwise we'll bias the samples and blahblahblah. Buuuuut we want to be able to open and close the notebook and have our results be repeatable. So here we usually want to pass in some arbitary random seed, so that each time the TTS is run *on the same dataset* you end up with the same random result. best practice is to pick a number and stick with it. Because programmers are nerds, 42 is a fairly widely used arbitrary seed. I do 14 because that's my lucky number. You can do whatever, it just forces the split to be the same each time you run it.\n",
    "\n",
    "##### shuffle \n",
    "Shuffle is a boolean option that defaults to true. Basically it makes sure that you're actually grabbing data points randomly, which we almost always want to do. \n",
    "\n",
    "##### stratify\n",
    "You can pass in an array to have the split try to ensure that it balances the split based on the contents of that array. Usually we don't do this, and if we do, usually we stratify by the target. \n",
    "\n",
    "For an example of when we might use this, if we have a classification dataset that has some very rare values in the target, we might want to force the t-t-s to try to make sure there's an equal proportion of each possible target in the train and the test. This is a problem that goes away with bigger datasets thanks to the Law of Large Numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our train_test_split\n",
    "\n",
    "##### arrays\n",
    "\n",
    "Our arrays will be the predictors on one hand and the target on the other. In this case, we'll use diabetes_df.drop('target', axis = 0) and diabetes_df.target\n",
    "\n",
    "##### test_size\n",
    "\n",
    "We can leave this as default.\n",
    "\n",
    "##### train_size\n",
    "\n",
    "We can leave this as default.\n",
    "\n",
    "##### random_state\n",
    "\n",
    "I'm going to set this at 14 just because I can.\n",
    "\n",
    "##### shuffle\n",
    "\n",
    "We can leave this as default.\n",
    "\n",
    "##### stratify\n",
    "\n",
    "We can leave this as default.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split(diabetes_df.drop('target', axis = 1), diabetes_df.target, random_state = 42)\n",
    "\n",
    "# This output is strange. train_test_split returns four arrays, so we need to set it to four different variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(diabetes_df.drop('target', axis = 1), diabetes_df.target, random_state = 42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_train.index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the way the indices line up. This is why indices are important!\n",
    "\n",
    "Note also how the indices have been randomized. \n",
    "\n",
    "Since our target was just a single column, y_train and y_test came out as a series while X_train and X_test came out as full dataframes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data leaking\n",
    "\n",
    "This leads to problems of data leakage, or, the possibility of, unless we're careful, information from the test set 'leaking' its way into our training data. This can happen a lot of ways, and we need to keep an eye out for it. \n",
    "\n",
    "For instance, check out [this paper](https://reproducible.cs.princeton.edu/), which is just some data scientists calling out other, seasoned and experienced academic data scientists for messing up data leakage. Note, that, kinda like the example we gave above, fixing the data leaks often led to the fancy shmancy algos performing only about as well as the simpler models.\n",
    "\n",
    "**We now need to take X_test and Y_test and hide them away. Lock the door, hide the key**\n",
    "\n",
    "We now want to do our pre-processing steps on X_train and y_train, and then be ready to, *once we've already built the model*, be able to reapply those steps to the test data.\n",
    "\n",
    "So what's some preprocessing we need to do?\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".Insert image of Dora the Explorer\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "That's right! We might want to scale this data, and we for sure need to do some one hot encoding for our categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are, as Jelly would say it 'in-STAUNCH-ee-a-ting' our ohe and scaler\n",
    "ohe1 = OneHotEncoder(sparse = False, drop = 'first', handle_unknown = 'ignore')\n",
    "scaler1 = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since for the life of me I can't track down what sex is supposed to map to, \n",
    "# I'm going with an arbitrary replacement.\n",
    "X_train.sex.replace({1: 'Jock', 2:'Nerd'}, inplace = True)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this fits the encoder on the column we need\n",
    "sex_ohe = ohe1.fit(X_train[['sex']])\n",
    "\n",
    "# this creates a new object by putting the encoder to the column,\n",
    "# then creates a DataFrame from that object, the label for the undropped column, \n",
    "# and the index from the training dataset\n",
    "\n",
    "sex_encoded = pd.DataFrame(sex_ohe.transform(X_train[['sex']]), columns=['Nerd'], index = X_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to add a column to our X_train dataset. \n",
    "\n",
    "X_train_encoded = pd.concat([X_train, sex_encoded['Nerd']], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm calling this just to spot check that the 'Nerd' column = 1 when sex = 'Nerd' and\n",
    "# Nerd = 0 when sex = 'Jock'\n",
    "# I could do this more rigourously but this works for now.\n",
    "X_train_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_encoded.drop(['sex'], axis = 1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since I know I'll need to repeat this later, I'll make a function for everything EXCEPT the fit step.\n",
    "\n",
    "def ohe_for_diabetes(data_df, fit_encoder):\n",
    "    data_df.sex.replace({1: 'Jock', 2:'Nerd'}, inplace = True)\n",
    "    data_sex_encoded = pd.DataFrame(fit_encoder.transform(data_df[['sex']]), columns=['Nerd'], index = data_df.index)\n",
    "    data_df_encoded = pd.concat([data_df, data_sex_encoded['Nerd']], axis = 1)\n",
    "    display(data_df_encoded)\n",
    "    ### I am including this so we can sanity check that \n",
    "    data_df_encoded.drop(['sex'], axis = 1, inplace= True)\n",
    "    return data_df_encoded\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't do this but I want to prove a point:\n",
    "\n",
    "X_test_encoded = ohe_for_diabetes(data_df = X_test, fit_encoder = sex_ohe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whew, that was kind of a pain, but we're set up for categorical columns now. On to scaling!\n",
    "\n",
    "### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back to some preprocessing\n",
    "# I got lazy here, can you spot it? It won't effect the model but it's still bad practice\n",
    "\n",
    "scaler1.fit(X_train_encoded)\n",
    "X_train_scaled = pd.DataFrame(scaler1.transform(X_train_encoded), \n",
    "                              columns=X_train_encoded.columns,\n",
    "                              index = X_train_encoded.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "\n",
    "model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = pd.DataFrame(scaler1.transform(X_test_encoded), \n",
    "                              columns=X_test_encoded.columns,\n",
    "                              index = X_test_encoded.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(model.score(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "sns.scatterplot(x = y_pred, y = y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(model.score(X_train_scaled, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(model, X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3 Code Challenge Review\n",
    "\n",
    "Made using resources from the Seattle team - thanks y'all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "* Gradient Descent\n",
    "* Logistic Regression\n",
    "* Classification Metrics\n",
    "* Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, plot_roc_curve\n",
    "from sklearn.tree import export_graphviz\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from src.call import call_on_students"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the data from 'auto-mpg.csv'\n",
    "mpg_df = pd.read_csv(\"data/auto-mpg.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the data\n",
    "mpg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot a simple linear regression line using just the horsepower column\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.regplot(x='horsepower', y='mpg', data=mpg_df, line_kws={\"color\":\"orange\"})\n",
    "plt.title('Relationship Between Horsepower and MPG')\n",
    "plt.xlim(0, 250)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above graph shows an approximate best fit line for the relationship between `horsepower` and `mpg` in our data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Describe the below chart: What is it showing? What does it tell us?\n",
    "\n",
    "![Slope-RSS relationship image](images/slope-rss-relationship.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer: \n",
    "\n",
    "- Plot shows the error (RSS) on the y-axis and the slope of the model on the x-axis\n",
    "- From this graph you can see that it arrived at about `m = -0.158` for the optimal coefficient value, since it's around that point that the error term (RSS) is smallest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Imagine that you're starting at a slope towards the top upper left corner. Using Zoom's annotate feature, demonstrate how gradient descent would work "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) What is a step size when talking about gradient descent? How does learning rate regulate step size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer: \n",
    "\n",
    "- Step size captures the amount to change the coefficient as it tries to minimize the error term\n",
    "- Learning rate determines how large those steps are to start\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Describe a logistic regression model:\n",
    "\n",
    "- What kind of target is a logistic regression model used for?\n",
    "- What are the predictions that a logistic regression model outputs?\n",
    "- How is it different from linear regression?\n",
    "- Is it a parametric or non-parametric model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer: \n",
    "\n",
    "- Used for classification problems (categorical targets)\n",
    "- Log-odds, which are translated into probabilities\n",
    "- Linear regression predicts a continuous target, and is not bound between 0 and 1\n",
    "- Parametric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Compare a logistic regression model to any of the other model types we've learned:\n",
    "\n",
    "- List one benefit of logistic regression when compared to the other model type\n",
    "- List one reason the other model type might be more useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer: \n",
    "\n",
    "- Benefit: simple to interpret, fits quickly, not prone to overfitting\n",
    "- Another model might be more useful if the target is imbalanced, or if there are interaction terms in the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Logistic Regression and Classification Metrics with Code\n",
    "\n",
    "### Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the same data, but now with a classification target\n",
    "mpg_class = pd.read_csv('data/auto-mpg-classification.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check this new dataframe out\n",
    "mpg_class.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Prepare our data for modeling:\n",
    "\n",
    "1. Perform a train/test split\n",
    "2. Scale the inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "# Set test_size=0.33 and random_state=42\n",
    "X = mpg_class.drop(columns='target')\n",
    "y = mpg_class['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Explore the `target` column and our model-less baseline\n",
    "\n",
    "1. What is the breakdown of the `target` column in our training data?\n",
    "2. What would a model-less baseline look like in this context?\n",
    "3. How accurate would that model-less understanding be on our test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1: explore the target column breakdown in train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to explore\n",
    "y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:\n",
    "\n",
    "- Imbalanced target - 74% of training data is in class 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2: What would a model-less baseline look like in this context?\n",
    "\n",
    "#### Answer:\n",
    "\n",
    "- Predicting only our majority class, 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3: How accurate would that baseline be on test data?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Code to find the answer\n",
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:\n",
    "\n",
    "- 75% accurate on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) What is one problem you could foresee based on this breakdown, and what is one strategy you could employ to address that problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:\n",
    "\n",
    "- Target is imbalanced\n",
    "- Oversampling, synthetic oversampling (SMOTE), set `class_weight`\n",
    "- Note that undersampling doesn't make sense here, since our dataset is so small\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9) Fit a logistic regression model, and plot a confusion matrix of the results on our test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a logistic regression model\n",
    "# Name the model `logreg` and set random_state = 42\n",
    "logreg = LogisticRegression(random_state=42)\n",
    "logreg.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a confusion matrix on the test data\n",
    "plot_confusion_matrix(logreg, X_test_scaled, y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10) Calculate the accuracy, precision, recall and f1-score for the test set\n",
    "\n",
    "You can use the confusion matrix above, or sklearn functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab predictions if using sklearn functions\n",
    "test_preds = logreg.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "# By hand: TP + TN / TP + TN + FP + FN\n",
    "accuracy = (23 + 97) / (23 + 97 + 1 + 9)\n",
    "print(accuracy)\n",
    "\n",
    "# Using sklearn\n",
    "accuracy = accuracy_score(y_test, test_preds)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision\n",
    "# By hand: TP / TP + FP\n",
    "precision = 23 / (23 + 1)\n",
    "print(precision)\n",
    "\n",
    "# Using sklearn\n",
    "precision = precision_score(y_test, test_preds)\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall\n",
    "# By hand: TP / TP + FN\n",
    "recall = 23 / (23 + 9)\n",
    "print(recall)\n",
    "\n",
    "# Using sklearn\n",
    "recall = recall_score(y_test, test_preds)\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1-Score\n",
    "# By hand\n",
    "f1score = 2 * precision * recall / (precision + recall)\n",
    "print(f1score)\n",
    "\n",
    "# Using sklearn\n",
    "f1score = f1_score(y_test, test_preds)\n",
    "print(f1score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11) Calculate the ROC-AUC on the test set, and plot the ROC curve\n",
    "\n",
    "For this you'll definitely want to use the sklearn functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate roc-auc\n",
    "# Need predicted probabilities\n",
    "test_probas = logreg.predict_proba(X_test_scaled)[:,1]\n",
    "\n",
    "roc_auc_score(y_test, test_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve\n",
    "plot_roc_curve(logreg, X_test_scaled, y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12) Evaluate! Based on the metrics of our test data, how is our model doing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:\n",
    "\n",
    "- Doing well! Very high metrics all around - more FN than FP (better precision than recall)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Decision Trees\n",
    "\n",
    "### Set Up\n",
    "\n",
    "Let's try a decision tree classifier. \n",
    "\n",
    "First, let's just have the tree split once, using just a single column. How would you set that up? Use random_state = 42."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create two different decision trees with a single split\n",
    "dt_maxdepth1_v1 = DecisionTreeClassifier(max_depth=1, random_state = 42)\n",
    "dt_maxdepth1_v2 = DecisionTreeClassifier(max_depth=1, random_state = 42)\n",
    "\n",
    "# Train the two trees on different columns\n",
    "\n",
    "# First fit dt_maxdepth1_v1 on 'weight', set it equal to dt_weight\n",
    "dt_weight = dt_maxdepth1_v1.fit(X_train[['weight']], y_train)\n",
    "\n",
    "# Then fit dt_maxdepth1_v2 on 'origin', set it equal to dt_origin\n",
    "dt_origin = dt_maxdepth1_v2.fit(X_train[['origin']], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Images:\n",
    "\n",
    "Here we've created two images of what the nodes should look like.\n",
    "\n",
    "| Version 1: Weight | Version 2: Origin |\n",
    "| ----------------- | ----------------- |  \n",
    "| ![max depth 1 - version 1](images/dt-maxdepth1-v1.png) | ![max depth 1 - version 2](images/dt-maxdepth1-v2.png) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13) Just looking at the images, which of these trees does a better job splitting the data? How can you tell?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:\n",
    "\n",
    "- The first DT produces more pure splits, thus is doing a better job of separating the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to check on your answer, let's try out just the default .score() for the models here.\n",
    "print(dt_weight.score(X_test[['weight']], y_test))\n",
    "\n",
    "print(dt_origin.score(X_test[['origin']], y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13 bonus) What's the default scoring metric for the sklearn DecisionTreeClassifier? Is it always the best metric to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:\n",
    "\n",
    "- The documentation says that the score will \"Return the mean accuracy on the given test data and labels\"\n",
    "- That is, it predicts for each set of x values fed in, and then checks that against each y. Then it averages the result. Although this is handy, there may be cases when average precision, recall, or F1 might be more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14) Fit a decision tree model, and plot a confusion matrix of the results on our test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a decision tree model\n",
    "# Name the model `dt` and set random_state = 42\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot a confusion matrix on the test data\n",
    "plot_confusion_matrix(dt, X_test, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the ROCs for the models we've done\n",
    "fig, ax = plt.subplots()\n",
    "plot_roc_curve(dt, X_test, y_test, ax=ax)\n",
    "plot_roc_curve(logreg, X_test_scaled, y_test, ax=ax)\n",
    "\n",
    "plt.title(\"Receiver Operating Characteristic Curves\\n(Evaluated on Test Set)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15) Which is the better model according to ROC-AUC score? How can you tell?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call_on_students(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer:\n",
    "\n",
    "- Logistic regression has the higher roc-auc score, and has more area under the curve since it's closer to the top left corner of the graph"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
